{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_content_between_chars(string: str, start=\"[\", end=\"]\"):\n",
    "    edited = []\n",
    "    skip = False\n",
    "    for char in string:\n",
    "        if char == start:\n",
    "            skip = True\n",
    "\n",
    "        if not skip:\n",
    "            edited.append(char)\n",
    "\n",
    "        if char == end:\n",
    "            assert skip, f\"Closing character found without opening in {''.join(edited)}\"\n",
    "            skip = False\n",
    "    return \"\".join(edited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from typing import List, Set\n",
    "\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.tokens import Span\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "def get_paragraphs(url: str):\n",
    "        page = urlopen(url)\n",
    "        assert page.getcode() == 200, f\"{url} is not valid\"\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        res = [soup.find(\"h1\")]\n",
    "        div = soup.find(\"div\", class_=\"mw-content-ltr mw-parser-output\")\n",
    "        res += [i for i in div.find_all(re.compile(\"(h.)|p\"))]\n",
    "\n",
    "        paragraphs = []\n",
    "        for tag in res:\n",
    "            if tag.name.startswith(\"h\"):\n",
    "                current_header = tag\n",
    "            elif tag.name == \"p\" and tag.get(\"style\") is None:\n",
    "                # remove multiple whitespaces\n",
    "                p = \" \".join(tag.text.split())\n",
    "                # remove additional whitespaces\n",
    "                p = p.strip()\n",
    "                # remove content between '[' and ']'\n",
    "                p = remove_content_between_chars(p)\n",
    "                if len(p) > 0:\n",
    "                    paragraphs.append({\n",
    "                        \"text\": p,\n",
    "                        \"header_name\": current_header.text,\n",
    "                        \"header_level\": current_header.name[-1]\n",
    "                    })\n",
    "        \n",
    "        return paragraphs\n",
    "        \n",
    "        \n",
    "def check_exact_match(paragraph: str, answer: str):\n",
    "    # case insensitive match\n",
    "    return bool(re.search(f\"(^|[^\\w]{{1}}){answer}($|[^\\w]{{1}})\", paragraph, flags=re.IGNORECASE))\n",
    "\n",
    "\n",
    "def check_full_match(paragraph: str, attribute: str, answer: str):\n",
    "    # a full match requires to match both attribute (e.g. President) and answers\n",
    "    for to_find in [attribute, answer]:\n",
    "        if not check_exact_match(paragraph, to_find):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def write_roman(num: int):\n",
    "    ROMAN = {\n",
    "        1000: \"M\",\n",
    "        900: \"CM\",\n",
    "        500: \"D\",\n",
    "        400: \"CD\",\n",
    "        100: \"C\",\n",
    "        90: \"XC\",\n",
    "        50: \"L\",\n",
    "        40: \"XL\",\n",
    "        10: \"X\",\n",
    "        9: \"IX\",\n",
    "        5: \"V\",\n",
    "        4: \"IV\",\n",
    "        1: \"I\",\n",
    "    }\n",
    "\n",
    "    def roman_num(num: int):\n",
    "        for r in ROMAN.keys():\n",
    "            x, y = divmod(num, r)\n",
    "            yield ROMAN[r] * x\n",
    "            num -= r * x\n",
    "            if num <= 0:\n",
    "                break\n",
    "\n",
    "    return \"\".join([a for a in roman_num(num)])\n",
    "\n",
    "\n",
    "def remove_additional_bits(string: str, additional_bits: List[str]):\n",
    "    for bit in additional_bits:\n",
    "        string = re.sub(bit, \"\", string)\n",
    "    return \" \".join(string.split())  # remove additional whitespaces\n",
    "\n",
    "\n",
    "def find_main_chunk(doc: Doc):\n",
    "    ancestor = None\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if ancestor is None:\n",
    "            ancestor = chunk\n",
    "        elif chunk.root.is_ancestor(ancestor.root):\n",
    "            ancestor = chunk.root\n",
    "    return ancestor\n",
    "\n",
    "\n",
    "def is_monarch(span: Span, monarch_nums: Set[str]):\n",
    "    for name_chunk in span.text.split():\n",
    "        if name_chunk in monarch_nums:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paragraphs for Spain: 100%|██████████| 150/150 [00:02<00:00, 57.52it/s]\n",
      "paragraphs for Stephen Curry: 100%|██████████| 100/100 [00:01<00:00, 62.79it/s]\n",
      "paragraphs for Toyota: 100%|██████████| 161/161 [00:02<00:00, 62.01it/s]\n",
      "paragraphs for Canada: 100%|██████████| 116/116 [00:01<00:00, 61.37it/s]\n",
      "paragraphs for Japan: 100%|██████████| 97/97 [00:01<00:00, 60.21it/s]\n",
      "paragraphs for Kevin Durant: 100%|██████████| 64/64 [00:00<00:00, 67.91it/s]\n",
      "paragraphs for CVS Health: 100%|██████████| 54/54 [00:00<00:00, 62.18it/s]\n",
      "paragraphs for Gazprom: 100%|██████████| 96/96 [00:01<00:00, 63.24it/s]\n",
      "paragraphs for Netherlands: 100%|██████████| 150/150 [00:02<00:00, 62.20it/s]\n",
      "paragraphs for Belgium: 100%|██████████| 101/101 [00:01<00:00, 58.17it/s]\n",
      "paragraphs for Singapore: 100%|██████████| 113/113 [00:01<00:00, 56.89it/s]\n",
      "paragraphs for Egypt: 100%|██████████| 205/205 [00:03<00:00, 58.14it/s]\n",
      "paragraphs for Karim Benzema: 100%|██████████| 85/85 [00:01<00:00, 60.50it/s]\n",
      "paragraphs for Harry Kane: 100%|██████████| 84/84 [00:01<00:00, 63.96it/s]\n",
      "paragraphs for Glencore: 100%|██████████| 62/62 [00:00<00:00, 63.17it/s]\n",
      "paragraphs for Australia: 100%|██████████| 109/109 [00:01<00:00, 57.68it/s]\n",
      "paragraphs for Italy: 100%|██████████| 210/210 [00:03<00:00, 57.74it/s]\n",
      "paragraphs for Kawhi Leonard: 100%|██████████| 57/57 [00:00<00:00, 63.13it/s]\n",
      "paragraphs for United Nations: 100%|██████████| 105/105 [00:01<00:00, 60.76it/s]\n",
      "paragraphs for Czech Republic: 100%|██████████| 150/150 [00:02<00:00, 58.26it/s]\n",
      "paragraphs for North Atlantic Treaty Organization: 100%|██████████| 57/57 [00:00<00:00, 62.60it/s]\n",
      "paragraphs for Paul George: 100%|██████████| 39/39 [00:00<00:00, 66.42it/s]\n",
      "paragraphs for Philippines: 100%|██████████| 110/110 [00:01<00:00, 57.43it/s]\n",
      "paragraphs for South Africa: 100%|██████████| 131/131 [00:02<00:00, 53.55it/s]\n",
      "paragraphs for United States of America: 100%|██████████| 107/107 [00:01<00:00, 59.36it/s]\n",
      "paragraphs for Saudi Arabia: 100%|██████████| 147/147 [00:02<00:00, 55.52it/s]\n",
      "paragraphs for Sergio Pérez: 100%|██████████| 100/100 [00:01<00:00, 71.63it/s]\n",
      "paragraphs for Romania: 100%|██████████| 115/115 [00:02<00:00, 57.37it/s]\n",
      "paragraphs for Mexico: 100%|██████████| 133/133 [00:02<00:00, 54.91it/s]\n",
      "paragraphs for Thailand: 100%|██████████| 170/170 [00:00<00:00, 6019.23it/s]\n",
      "paragraphs for South Korea: 100%|██████████| 144/144 [00:02<00:00, 59.27it/s]\n",
      "paragraphs for Shell: 100%|██████████| 1/1 [00:00<00:00, 50.70it/s]\n",
      "paragraphs for Thailand: 100%|██████████| 170/170 [00:02<00:00, 57.94it/s]\n",
      "paragraphs for Walmart: 100%|██████████| 191/191 [00:03<00:00, 62.41it/s]\n",
      "paragraphs for Iran: 100%|██████████| 207/207 [00:03<00:00, 59.35it/s]\n",
      "paragraphs for Iraq: 100%|██████████| 188/188 [00:03<00:00, 55.28it/s]\n",
      "paragraphs for Norway: 100%|██████████| 193/193 [00:03<00:00, 56.15it/s]\n",
      "paragraphs for Russia: 100%|██████████| 143/143 [00:02<00:00, 62.06it/s]\n",
      "paragraphs for Apple: 100%|██████████| 76/76 [00:01<00:00, 63.12it/s]\n",
      "paragraphs for World Bank: 100%|██████████| 86/86 [00:01<00:00, 58.80it/s]\n",
      "paragraphs for Canada: 100%|██████████| 116/116 [00:01<00:00, 58.54it/s]\n",
      "paragraphs for Portugal: 100%|██████████| 164/164 [00:03<00:00, 54.45it/s]\n",
      "paragraphs for Charles Leclerc: 100%|██████████| 66/66 [00:00<00:00, 16188.54it/s]\n",
      "paragraphs for Chile: 100%|██████████| 157/157 [00:02<00:00, 60.97it/s]\n",
      "paragraphs for United Arab Emirates: 100%|██████████| 138/138 [00:02<00:00, 51.47it/s]\n",
      "paragraphs for Pierre Gasly: 100%|██████████| 45/45 [00:00<00:00, 18308.63it/s]\n",
      "paragraphs for Germany: 100%|██████████| 95/95 [00:01<00:00, 53.79it/s]\n",
      "paragraphs for China: 100%|██████████| 128/128 [00:02<00:00, 60.15it/s]\n",
      "paragraphs for Ireland: 100%|██████████| 148/148 [00:02<00:00, 59.46it/s]\n",
      "paragraphs for TotalEnergies: 100%|██████████| 89/89 [00:01<00:00, 58.04it/s]\n",
      "paragraphs for Argentina: 100%|██████████| 150/150 [00:02<00:00, 59.60it/s]\n",
      "paragraphs for Kylian Mbappé: 100%|██████████| 61/61 [00:00<00:00, 62.18it/s]\n",
      "paragraphs for BP: 100%|██████████| 175/175 [00:02<00:00, 59.11it/s]\n",
      "paragraphs for Poland: 100%|██████████| 134/134 [00:02<00:00, 59.91it/s]\n",
      "paragraphs for Czech Republic: 100%|██████████| 150/150 [00:02<00:00, 63.52it/s]\n",
      "paragraphs for Fernando Alonso: 100%|██████████| 80/80 [00:01<00:00, 63.77it/s]\n",
      "paragraphs for Vietnam: 100%|██████████| 109/109 [00:02<00:00, 53.77it/s]\n",
      "paragraphs for Ford Motor Company: 100%|██████████| 151/151 [00:02<00:00, 61.29it/s]\n",
      "paragraphs for Brazil: 100%|██████████| 163/163 [00:02<00:00, 54.54it/s]\n",
      "paragraphs for Microsoft: 100%|██████████| 87/87 [00:01<00:00, 63.49it/s]\n",
      "paragraphs for Russia: 100%|██████████| 143/143 [00:02<00:00, 58.90it/s]\n",
      "paragraphs for Iraq: 100%|██████████| 188/188 [00:03<00:00, 55.71it/s]\n",
      "paragraphs for Volkswagen Group: 100%|██████████| 54/54 [00:00<00:00, 60.91it/s]\n",
      "paragraphs for Netherlands: 100%|██████████| 150/150 [00:02<00:00, 57.04it/s]\n",
      "paragraphs for Colombia: 100%|██████████| 145/145 [00:02<00:00, 59.91it/s]\n",
      "paragraphs for Nigeria: 100%|██████████| 154/154 [00:02<00:00, 59.94it/s]\n",
      "paragraphs for Erling Haaland: 100%|██████████| 50/50 [00:00<00:00, 76.77it/s]\n",
      "paragraphs for Carlos Sainz Jr.: 100%|██████████| 48/48 [00:00<00:00, 4032.18it/s]\n",
      "paragraphs for Romania: 100%|██████████| 115/115 [00:01<00:00, 57.83it/s]\n",
      "paragraphs for Amazon: 100%|██████████| 2/2 [00:00<00:00, 51.73it/s]\n",
      "paragraphs for France: 100%|██████████| 162/162 [00:02<00:00, 59.17it/s]\n",
      "paragraphs for Finland: 100%|██████████| 151/151 [00:02<00:00, 59.13it/s]\n",
      "paragraphs for LeBron James: 100%|██████████| 100/100 [00:01<00:00, 61.18it/s]\n",
      "paragraphs for Pakistan: 100%|██████████| 171/171 [00:03<00:00, 52.21it/s]\n",
      "paragraphs for Spain: 100%|██████████| 150/150 [00:02<00:00, 57.41it/s]\n",
      "paragraphs for Lewis Hamilton: 100%|██████████| 133/133 [00:00<00:00, 5410.43it/s]\n",
      "paragraphs for Mitsubishi: 100%|██████████| 22/22 [00:00<00:00, 58.14it/s]\n",
      "paragraphs for Australia: 100%|██████████| 109/109 [00:01<00:00, 60.91it/s]\n",
      "paragraphs for Egypt: 100%|██████████| 205/205 [00:03<00:00, 57.85it/s]\n",
      "paragraphs for Austria: 100%|██████████| 148/148 [00:02<00:00, 57.20it/s]\n",
      "paragraphs for Poland: 100%|██████████| 134/134 [00:02<00:00, 60.76it/s]\n",
      "paragraphs for Denmark: 100%|██████████| 130/130 [00:02<00:00, 58.92it/s]\n",
      "paragraphs for United Kingdom: 100%|██████████| 147/147 [00:02<00:00, 56.87it/s]\n",
      "paragraphs for United Kingdom: 100%|██████████| 147/147 [00:02<00:00, 61.07it/s]\n",
      "paragraphs for France: 100%|██████████| 162/162 [00:02<00:00, 61.26it/s]\n",
      "paragraphs for North Atlantic Treaty Organization: 100%|██████████| 57/57 [00:00<00:00, 60.96it/s]\n",
      "paragraphs for Germany: 100%|██████████| 95/95 [00:01<00:00, 61.41it/s]\n",
      "paragraphs for Finland: 100%|██████████| 151/151 [00:02<00:00, 58.47it/s]\n",
      "paragraphs for Sweden: 100%|██████████| 172/172 [00:03<00:00, 57.02it/s]\n",
      "paragraphs for International Olympic Committee: 100%|██████████| 80/80 [00:01<00:00, 63.67it/s]\n",
      "paragraphs for Turkey: 100%|██████████| 157/157 [00:02<00:00, 55.56it/s]\n",
      "paragraphs for Cristiano Ronaldo: 100%|██████████| 130/130 [00:00<00:00, 10850.29it/s]\n",
      "paragraphs for Israel: 100%|██████████| 142/142 [00:02<00:00, 61.33it/s]\n",
      "paragraphs for Bradley Beal: 100%|██████████| 42/42 [00:00<00:00, 70.20it/s]\n",
      "paragraphs for Sadio Mané: 100%|██████████| 61/61 [00:00<00:00, 18089.12it/s]\n",
      "paragraphs for Cencora, Inc.: 100%|██████████| 19/19 [00:00<00:00, 57.39it/s]\n",
      "paragraphs for Bangladesh: 100%|██████████| 139/139 [00:02<00:00, 61.42it/s]\n",
      "paragraphs for Vietnam: 100%|██████████| 109/109 [00:02<00:00, 42.91it/s]\n",
      "paragraphs for Mohamed Salah: 100%|██████████| 84/84 [00:00<00:00, 4805.85it/s]\n",
      "paragraphs for Giannis Antetokounmpo: 100%|██████████| 53/53 [00:00<00:00, 67.21it/s]\n",
      "paragraphs for Alphabet Inc.: 100%|██████████| 40/40 [00:00<00:00, 63.29it/s]\n",
      "paragraphs for Lando Norris: 100%|██████████| 44/44 [00:00<00:00, 21144.52it/s]\n",
      "paragraphs for Japan: 100%|██████████| 97/97 [00:00<00:00, 5680.01it/s]\n",
      "paragraphs for Turkey: 100%|██████████| 157/157 [00:02<00:00, 58.24it/s]\n",
      "paragraphs for Saudi Aramco: 100%|██████████| 75/75 [00:01<00:00, 56.73it/s]\n",
      "paragraphs for Neymar Jr.: 100%|██████████| 142/142 [00:00<00:00, 15151.14it/s]\n",
      "paragraphs for India: 100%|██████████| 102/102 [00:01<00:00, 60.77it/s]\n",
      "paragraphs for Pakistan: 100%|██████████| 171/171 [00:02<00:00, 58.89it/s]\n",
      "paragraphs for Bangladesh: 100%|██████████| 139/139 [00:02<00:00, 59.34it/s]\n",
      "paragraphs for Austria: 100%|██████████| 148/148 [00:02<00:00, 62.34it/s]\n",
      "paragraphs for Norway: 100%|██████████| 193/193 [00:03<00:00, 58.85it/s]\n",
      "paragraphs for Portugal: 100%|██████████| 164/164 [00:02<00:00, 56.45it/s]\n",
      "paragraphs for International Atomic Energy Agency: 100%|██████████| 52/52 [00:00<00:00, 56.38it/s]\n",
      "paragraphs for Denmark: 100%|██████████| 130/130 [00:02<00:00, 58.59it/s]\n",
      "paragraphs for Italy: 100%|██████████| 210/210 [00:03<00:00, 59.31it/s]\n",
      "paragraphs for Israel: 100%|██████████| 142/142 [00:02<00:00, 59.27it/s]\n",
      "paragraphs for George Russell: 100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "paragraphs for ExxonMobil: 100%|██████████| 43/43 [00:00<00:00, 60.24it/s]\n",
      "paragraphs for Malaysia: 100%|██████████| 80/80 [00:01<00:00, 61.43it/s]\n",
      "paragraphs for China: 100%|██████████| 128/128 [00:02<00:00, 63.50it/s]\n",
      "paragraphs for Max Verstappen: 100%|██████████| 81/81 [00:00<00:00, 82.82it/s]\n",
      "paragraphs for Indonesia: 100%|██████████| 94/94 [00:01<00:00, 60.66it/s]\n",
      "paragraphs for Kevin De Bruyne: 100%|██████████| 68/68 [00:00<00:00, 85.66it/s]\n",
      "paragraphs for Nikola Jokic: 100%|██████████| 50/50 [00:00<00:00, 71.50it/s]\n",
      "paragraphs for India: 100%|██████████| 102/102 [00:01<00:00, 60.32it/s]\n",
      "paragraphs for Lionel Messi: 100%|██████████| 169/169 [00:02<00:00, 62.52it/s]\n",
      "paragraphs for Belgium: 100%|██████████| 101/101 [00:01<00:00, 59.87it/s]\n",
      "paragraphs for Damian Lillard: 100%|██████████| 48/48 [00:00<00:00, 61.63it/s]\n",
      "paragraphs for Iran: 100%|██████████| 207/207 [00:03<00:00, 60.23it/s]\n",
      "paragraphs for Sweden: 100%|██████████| 172/172 [00:02<00:00, 59.00it/s]\n",
      "100%|██████████| 130/130 [05:14<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "MONARCH_NUMS = {write_roman(i) for i in range(1, 100, 1)}\n",
    "\n",
    "ADDITIONAL_BITS = [\n",
    "    \"[\\w]?F(\\.)?C(\\.)?[\\w]?\",  # FC, F.C., AFC, ... for Football\n",
    "    \"[\\w]?C(\\.)?F(\\.)?[\\w]?\",  # CF, ... for Football\n",
    "    \"[\\w]?F(\\.)?K(\\.)?[\\w]?\",  # FK, ... for Football\n",
    "    \"[\\w]?A(\\.)?S(\\.)?[\\w]?\",  # AS, ... for Football\n",
    "    \"[\\w]?S(\\.)?V(\\.)?[\\w]?\",  # SV, ... for Football\n",
    "    \"[\\w]?B(\\.)?C(\\.)?[\\w]?\",  # BC, ... for Basketball\n",
    "    \"[\\w](\\.)[\\w](\\.)\",  # General regex for to remove two letter acronyms (with )\n",
    "    \"football\",\n",
    "    \"(t|T)eam\",\n",
    "    \"association\",\n",
    "    \"men's\",\n",
    "    \"basketball\",\n",
    "    \"F1\",\n",
    "    \"(S|s)cuderia\",\n",
    "    \"(R|r)acing\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab)  # Whitespace tokenization\n",
    "\n",
    "passages = {}\n",
    "with open(\"wikipedia_pages.csv\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"wikipedia_pages.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "\n",
    "    for row in tqdm(reader, total=len(lines)-1):\n",
    "        category = row[\"category\"]\n",
    "        item = row[\"item\"]\n",
    "        page_url = row[\"page_url\"]\n",
    "        attribute = row[\"attribute\"]\n",
    "        attribute = attribute if len(attribute) > 0 else None\n",
    "        \n",
    "        if category not in passages:\n",
    "            passages[category] = {}\n",
    "        \n",
    "        if item not in passages[category]:\n",
    "            passages[category][item] = {}\n",
    "\n",
    "        if attribute is not None:\n",
    "            if attribute not in passages[category][item]:\n",
    "                passages[category][item][attribute] = {}\n",
    "\n",
    "        # remove unicode characters from url\n",
    "        url = unidecode(page_url)\n",
    "        paragraphs = get_paragraphs(url)\n",
    "\n",
    "        matches = {\n",
    "            \"full\": [],\n",
    "            \"em\": [],\n",
    "            \"simplified\": [],\n",
    "            \"head\": [],\n",
    "        }\n",
    "\n",
    "        no_match = True\n",
    "        for paragraph in tqdm(paragraphs, desc=f\"paragraphs for {item}\"):\n",
    "            answer = row[\"answer\"]\n",
    "            append_to = None\n",
    "            matched = None\n",
    "\n",
    "            attr = attribute\n",
    "            if attribute is not None:\n",
    "                if \"prime minister\" in attr.lower():\n",
    "                    attr = \"prime minister\"\n",
    "                if \"president\" in attr.lower():\n",
    "                    attr = \"president\"\n",
    "                if \"king\" in attr.lower():\n",
    "                    attr = \"king\"\n",
    "                if \"monarch\" in attr.lower():\n",
    "                    attr = \"monarch\"\n",
    "                if \"supreme leader\" in attr.lower():\n",
    "                    attr = \"supreme leader\"\n",
    "                if \"premier\" in attr.lower():\n",
    "                    attr = \"premier\"\n",
    "            if check_full_match(paragraph[\"text\"], attr, answer):\n",
    "                append_to = \"full\"\n",
    "                matched = (attr, answer)\n",
    "                no_match = False\n",
    "            elif check_exact_match(paragraph[\"text\"], answer):\n",
    "                append_to = \"em\"\n",
    "                matched = answer\n",
    "                no_match = False\n",
    "            else:\n",
    "                if category in [\"athletes_byPayment\"]:\n",
    "                    answer = remove_additional_bits(answer, ADDITIONAL_BITS)\n",
    "\n",
    "                if check_exact_match(paragraph[\"text\"], answer):\n",
    "                    append_to = \"simplified\"\n",
    "                    matched = answer\n",
    "                    no_match = False\n",
    "                elif len(answer.split()) > 1:\n",
    "                    doc = nlp(answer)\n",
    "                    main_chunk = find_main_chunk(doc)\n",
    "\n",
    "                    if main_chunk is not None:\n",
    "                        if is_monarch(main_chunk, MONARCH_NUMS):\n",
    "                            answer = main_chunk.text\n",
    "                        else:\n",
    "                            answer = main_chunk.root.text\n",
    "\n",
    "                    if check_exact_match(paragraph[\"text\"], answer):\n",
    "                        append_to = \"head\"\n",
    "                        #print(answer)\n",
    "                        matched = answer\n",
    "                        no_match = False\n",
    "\n",
    "            \n",
    "            if append_to:\n",
    "                matches[append_to].append({\n",
    "                    \"paragraph\": paragraph,\n",
    "                    \"matched\": matched\n",
    "                })\n",
    "\n",
    "        if attribute is not None:\n",
    "            if attribute not in passages[category][item]:\n",
    "                passages[category][item][attribute] = {}\n",
    "\n",
    "            passages[category][item][attribute] = {\n",
    "                \"matches\": matches,\n",
    "                \"page_url\": url,\n",
    "                \"no_match\": no_match\n",
    "            }\n",
    "        else:\n",
    "            passages[category][item] = {\n",
    "                \"matches\": matches,\n",
    "                \"page_url\": url,\n",
    "                \"no_match\": no_match\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "with open(\"passages.json\", \"w\") as f:\n",
    "    json.dump(passages, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:  130\n",
      "AFTER:  79\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "with open(\"passages.json\", \"r\") as f:\n",
    "    passages = json.load(f)\n",
    "\n",
    "outdated_questions = {}\n",
    "for folder in Path(\"/home/simone/papers/ACL/knowledge-editing/models_editing/editing_datasets\").iterdir():\n",
    "    editing_dataset = os.path.join(folder, \"editing_dataset.json\")\n",
    "    with open(editing_dataset, \"r\") as f:\n",
    "        editing_dataset = json.load(f)\n",
    "    for sample in editing_dataset:\n",
    "        domain = sample[\"domain\"]\n",
    "        element = sample[\"element\"]\n",
    "        attribute = sample[\"attribute\"]\n",
    "        if domain not in outdated_questions:\n",
    "            outdated_questions[domain] = {}\n",
    "        if element not in outdated_questions[domain]:\n",
    "            outdated_questions[domain][element] = {}\n",
    "        if attribute is not None:\n",
    "            if  element not in outdated_questions[domain][element]:\n",
    "                outdated_questions[domain][element][attribute] = {}\n",
    "\n",
    "def count_questions(passages):\n",
    "    n_questions = 0\n",
    "    for domain in passages:\n",
    "        for element in passages[domain]:\n",
    "            if domain in [\"countries_byGDP\", \"organizations\"]:\n",
    "                for attribute in passages[domain][element]:\n",
    "                    n_questions += 1\n",
    "            else:\n",
    "                n_questions += 1\n",
    "    return n_questions\n",
    "\n",
    "print(\"BEFORE: \", count_questions(passages))\n",
    "passages_copy = deepcopy(passages)\n",
    "\n",
    "for domain in passages_copy:\n",
    "    for element in passages_copy[domain]:\n",
    "        if element not in outdated_questions[domain]:\n",
    "            del passages[domain][element]\n",
    "        else:\n",
    "            if domain in [\"countries_byGDP\", \"organizations\"]:\n",
    "                for attribute in passages_copy[domain][element]:\n",
    "                    if attribute not in outdated_questions[domain][element]:\n",
    "                        del passages[domain][element][attribute]\n",
    "\n",
    "        \n",
    "print(\"AFTER: \", count_questions(passages))\n",
    "        \n",
    "    \n",
    "with open(\"editing_passages.json\", \"w\") as f:\n",
    "    json.dump(passages, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"editing_passages.json\", \"r\") as f:\n",
    "    editing_passages = json.load(f)\n",
    "\n",
    "passages_per_domain = {}\n",
    "for domain in editing_passages:\n",
    "    if domain not in passages_per_domain:\n",
    "        passages_per_domain[domain] = []\n",
    "    for element in editing_passages[domain]:\n",
    "        if domain in [\"countries_byGDP\", \"organizations\"]:\n",
    "            for attribute in editing_passages[domain][element]:\n",
    "                question_passages = editing_passages[domain][element][attribute]\n",
    "\n",
    "                matches = []\n",
    "                for matches_per_category in question_passages[\"matches\"].values():\n",
    "                    matches += [m[\"paragraph\"][\"text\"] for m in matches_per_category]\n",
    "\n",
    "                assert len(matches) == 1, f\"You should have only 1 passage for each question but you have {len(matches)} for {domain} -- {element} -- {attribute}\"\n",
    "                context = matches.pop()\n",
    "\n",
    "                passages_per_domain[domain].append(context)\n",
    "\n",
    "        else:\n",
    "            question_passages = editing_passages[domain][element]\n",
    "\n",
    "            matches = []\n",
    "            for matches_per_category in question_passages[\"matches\"].values():\n",
    "                matches += [m[\"paragraph\"][\"text\"] for m in matches_per_category]\n",
    "\n",
    "            assert len(matches) == 1, f\"You should have only 1 passage for each question but you have {len(matches)} for {domain} -- {element}\"\n",
    "            context = matches.pop()\n",
    "\n",
    "            passages_per_domain[domain].append(context)\n",
    "\n",
    "#for domain, ps in passages_per_domain.items():\n",
    "#    print(domain, len(ps))\n",
    "#print()\n",
    "\n",
    "# set the seed once at the beginning\n",
    "random.seed(42)\n",
    "edited_passages = {}\n",
    "for domain in editing_passages:\n",
    "    # take the passages that do not belong to this domain\n",
    "    noisy_passages = [p for d, ps in passages_per_domain.items() for p in ps if d != domain]\n",
    "    #print(domain, len(noisy_passages))\n",
    "    for element in editing_passages[domain]:\n",
    "        if domain in [\"countries_byGDP\", \"organizations\"]:\n",
    "            for attribute in editing_passages[domain][element]:\n",
    "                question_passages = editing_passages[domain][element][attribute]\n",
    "\n",
    "                matches = []\n",
    "                for matches_per_category in question_passages[\"matches\"].values():\n",
    "                    matches += [m[\"paragraph\"][\"text\"] for m in matches_per_category]\n",
    "\n",
    "                assert len(matches) == 1, f\"You should have only 1 passage for each question but you have {len(matches)} for {domain} -- {element} -- {attribute}\"\n",
    "                \n",
    "                for matches_per_category in question_passages[\"matches\"].values():\n",
    "                    if len(matches_per_category) == 1:\n",
    "                        noisy_p = random.choice(noisy_passages)\n",
    "                        p = matches_per_category[0][\"paragraph\"][\"text\"]\n",
    "                        p = \"\\n\".join([noisy_p, p])\n",
    "                        matches_per_category[0][\"paragraph\"][\"text\"] = p\n",
    "                        break\n",
    "        else:\n",
    "            question_passages = editing_passages[domain][element]\n",
    "\n",
    "            matches = []\n",
    "            for matches_per_category in question_passages[\"matches\"].values():\n",
    "                matches += [m[\"paragraph\"][\"text\"] for m in matches_per_category]\n",
    "\n",
    "            assert len(matches) == 1, f\"You should have only 1 passage for each question but you have {len(matches)} for {domain} -- {element}\"\n",
    "\n",
    "            for matches_per_category in question_passages[\"matches\"].values():\n",
    "                if len(matches_per_category) == 1:\n",
    "                    noisy_p = random.choice(noisy_passages)\n",
    "                    p = matches_per_category[0][\"paragraph\"][\"text\"]\n",
    "                    p = \"\\n\".join([noisy_p, p])\n",
    "                    matches_per_category[0][\"paragraph\"][\"text\"] = p\n",
    "                    break\n",
    "\n",
    "\n",
    "with open(\"noisy_editing_passages.json\", \"w\") as f:\n",
    "    json.dump(editing_passages, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"editing_passages.json\", \"r\") as f:\n",
    "    editing_passages = json.load(f)\n",
    "\n",
    "passages_per_domain = {}\n",
    "for domain in editing_passages:\n",
    "    if domain not in passages_per_domain:\n",
    "        passages_per_domain[domain] = []\n",
    "    for element in editing_passages[domain]:\n",
    "        if domain in [\"countries_byGDP\", \"organizations\"]:\n",
    "            for attribute in editing_passages[domain][element]:\n",
    "                question_passages = editing_passages[domain][element][attribute]\n",
    "\n",
    "                matches = []\n",
    "                for matches_per_category in question_passages[\"matches\"].values():\n",
    "                    matches += [m[\"paragraph\"][\"text\"] for m in matches_per_category]\n",
    "\n",
    "                assert len(matches) == 1, f\"You should have only 1 passage for each question but you have {len(matches)} for {domain} -- {element} -- {attribute}\"\n",
    "                context = matches.pop()\n",
    "\n",
    "                passages_per_domain[domain].append(context)\n",
    "\n",
    "        else:\n",
    "            question_passages = editing_passages[domain][element]\n",
    "\n",
    "            matches = []\n",
    "            for matches_per_category in question_passages[\"matches\"].values():\n",
    "                matches += [m[\"paragraph\"][\"text\"] for m in matches_per_category]\n",
    "\n",
    "            assert len(matches) == 1, f\"You should have only 1 passage for each question but you have {len(matches)} for {domain} -- {element}\"\n",
    "            context = matches.pop()\n",
    "\n",
    "            passages_per_domain[domain].append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_passages = [p for ps in passages_per_domain.values() for p in ps]\n",
    "random.seed(42)\n",
    "random.shuffle(all_passages)\n",
    "\n",
    "with open(\"langchain/data/passages.txt\", \"w\") as f:\n",
    "    for p in all_passages:\n",
    "        f.write(p + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
